{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "674755de-d2f6-4a0e-ba8d-0541cd84994f",
   "metadata": {},
   "source": [
    "# 0. Configuration Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621970fa-b5f4-4012-aa50-fc4bcd00f7e2",
   "metadata": {},
   "source": [
    "remarque : on suit [le tp de formation à spark du ssplab](https://github.com/InseeFrLab/formation-spark/tree/main/5-hive-metastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed93341-ee16-4103-bcc6-0e345e4b9ebc",
   "metadata": {},
   "source": [
    "# 1. lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b037faa5-b518-49dd-aa52-79a19455b55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import s3fs\n",
    "import json\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "\n",
    "\n",
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .load(\"s3a://projet-si-collecte/json.txt\")\n",
    "     )\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a6b989-0539-4374-818c-5d41455a0564",
   "metadata": {},
   "source": [
    "Problème de schéma, le fichier n'est pas en format Json classique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "da99bc80-c914-49d3-a01a-5c3b7e400540",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open(\"json100000.txt\",\"r\")\n",
    "lines = f.readlines()\n",
    "data=[line.replace(\"'\",\"\\\"\") for line in lines]\n",
    "data=[line.replace(\"True\",\"\\\"True\\\"\") for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e1df5660-e793-4a82-8dbe-d379708dd2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "df = (spark.read\n",
    "      .format(\"json\")\n",
    "     # .options(header='false', inferschema='false', delimiter=\"\\n\")\n",
    "      .load(\"s3a://projet-si-collecte/json3.txt\")\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6403465b-1733-4fba-bd04-20686f7e6da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- COLLECTED: struct (nullable = true)\n",
      " |    |-- COMMENT: string (nullable = true)\n",
      " |    |-- PRODUCER: string (nullable = true)\n",
      " |    |-- READY: string (nullable = true)\n",
      " |-- EXTERNAL: struct (nullable = true)\n",
      " |    |-- LAST_BROADCAST: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e5ac67-674a-4900-9426-f250bc5a43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Le schéma est réparé. On peut mettre le fichier dans Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b9db64-8998-4c71-92e8-c4c61e1b875d",
   "metadata": {},
   "source": [
    "# 2. mettre la BDD dans HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1de28aaa-0a01-4182-acb5-4c6764c29902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show databases;').show()\n",
    "# Pour l'instant on n'a pas de table SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd36ba6-e9c4-45be-a940-4fa9d8a6a4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "2022-06-15 09:53:48,983 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-06-15 09:53:51,583 WARN spark.ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "### On réutilise l'objet conf des autres tutoriels mais on lance la session spark avec le builder de sparksession pour activer le support de Hive\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "conf = SparkConf()\n",
    "\n",
    "#url par défaut d'une api kubernetes accédé depuis l'intérieur du cluster (ici le notebook tourne lui même dans kubernetes)\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc:443\")\n",
    "\n",
    "#image des executors spark: pour des raisons de simplicité on réutilise l'image du notebook\n",
    "conf.set(\"spark.kubernetes.container.image\",  os.environ['IMAGE_NAME'])\n",
    "\n",
    "# Nom du compte de service pour contacter l'api kubernetes : attention le package du datalab crée lui même cette variable d'enviromment.\n",
    "# Dans un pod du cluster kubernetes il faut lire le fichier /var/run/secrets/kubernetes.io/serviceaccount/token\n",
    "# Néanmoins ce paramètre est inutile car le contexte kubernetes local de ce notebook est préconfiguré\n",
    "# conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \n",
    "\n",
    "# Nom du namespace kubernetes\n",
    "conf.set(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\n",
    "\n",
    "# Nombre d'executeur spark, il se lancera autant de pods kubernetes que le nombre indiqué.\n",
    "conf.set(\"spark.executor.instances\", \"5\")\n",
    "\n",
    "# Mémoire alloué à la JVM\n",
    "# Attention par défaut le pod kubernetes aura une limite supérieur qui dépend d'autres paramètres.\n",
    "# On manipulera plus bas pour vérifier la limite de mémoire totale d'un executeur\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "\n",
    "conf.set(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"5-Hive-metastore\").config(conf = conf).enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297bf82-a844-4fc5-93ed-145515e9b9b4",
   "metadata": {},
   "source": [
    "## 2.1 Récupérer le schéma de la base pour la requête sparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2de9b7e-fa77-4ba2-8eee-8ffcfbd2e0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- COLLECTED: struct (nullable = true)\n",
      " |    |-- COMMENT: string (nullable = true)\n",
      " |    |-- PRODUCER: string (nullable = true)\n",
      " |    |-- READY: string (nullable = true)\n",
      " |-- EXTERNAL: struct (nullable = true)\n",
      " |    |-- LAST_BROADCAST: string (nullable = true)\n",
      "\n",
      "\n",
      "---------print definition ---------\n",
      "\n",
      "CREATE EXTERNAL TABLE IF NOT EXISTS queen2 (COLLECTED struct<COMMENT:string,PRODUCER:string,READY:string>,EXTERNAL struct<LAST_BROADCAST:string> ) STORED as parquet LOCATION 's3a://projet-si-collecte/table/data'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SQLContext\n",
    "df = (spark.read\n",
    "      .format(\"json\")\n",
    "     # .options(header='false', inferschema='false', delimiter=\"\\n\")\n",
    "      .load(\"s3a://projet-si-collecte/json3.txt\")\n",
    "     )\n",
    "df.printSchema()\n",
    "cols = df.dtypes\n",
    "buf = []\n",
    "buf.append('CREATE EXTERNAL TABLE IF NOT EXISTS queen2 (')\n",
    "keyanddatatypes =  df.dtypes\n",
    "sizeof = len(df.dtypes)\n",
    "count=1;\n",
    "for eachvalue in keyanddatatypes:\n",
    "    if count == sizeof:\n",
    "        total = str(eachvalue[0])+str(' ')+str(eachvalue[1])\n",
    "    else:\n",
    "        total = str(eachvalue[0]) + str(' ') + str(eachvalue[1]) + str(',')\n",
    "    buf.append(total)\n",
    "    count = count + 1\n",
    "\n",
    "buf.append(' )')\n",
    "buf.append(' STORED as parquet ')\n",
    "buf.append(\"LOCATION \")\n",
    "buf.append(\"'\")\n",
    "buf.append('s3a://projet-si-collecte/table/data')\n",
    "buf.append(\"'\")\n",
    "\n",
    "##partition by pt\n",
    "tabledef = ''.join(buf)\n",
    "\n",
    "print (\"\\n---------print definition ---------\\n\")\n",
    "print (tabledef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e897bea-d44f-43f1-9c71-91c27bbb3476",
   "metadata": {},
   "source": [
    "# 2.2 Lancer la requête SQL pour créer la table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d6f5113-7b50-4b68-a9f0-1da815aaa912",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(tabledef);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e557a51-702f-4ac0-a8a8-e539caa5d42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default|    queen|      false|\n",
      "|  default|   queen2|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62e567dd-6978-4045-8823-ed5628a13e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/dataframe.py:138: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d1ec4a-2ad1-45bb-b5ee-ccc1255d6fd3",
   "metadata": {},
   "source": [
    "# 3. La base est dans HIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117da3a3-efb5-4c94-bc6d-061237d0a2dd",
   "metadata": {},
   "source": [
    "# 4. Créer un trino\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbefe72-28c2-419e-99ab-6d69e3f40952",
   "metadata": {},
   "source": [
    "## 4.1 Configurer le trino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94154aa-7074-4db5-a772-72219d2a1fc2",
   "metadata": {},
   "source": [
    "Décocher dans `security` le `Enable IP protection - Only the configured set of IPs will be able to reach the service` pour que trino soit accessible par superset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef0247-3b17-4960-9271-abd63f116ec9",
   "metadata": {},
   "source": [
    "# 5. Créer un superset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e347a6-ec66-4a33-8262-c94e3fca2ea7",
   "metadata": {},
   "source": [
    "## 5.1 charger la BDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f581c1-06fe-4bbc-952c-50ebb5fb8d5c",
   "metadata": {},
   "source": [
    "Aller dans le menu `Data` > `Datasets`\n",
    "\n",
    "Cliquer en haut à droite sur `+ DATASET`\n",
    "\n",
    "Choisir le bon `DATASET` (un qui commence par `hive-...`)\n",
    "\n",
    "Choisir le bon `SCHEMA` (`default`)\n",
    "\n",
    "Choisir la bonne `TABLE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d79c5a-27b0-4dc1-b064-ad596aec1997",
   "metadata": {},
   "source": [
    "# 5.2 Créer le dashboard + chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260dad5-9df2-4396-b477-0d7a1008bd14",
   "metadata": {},
   "source": [
    "TODO : ajouter les étapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be846d-3cfe-4282-8933-1895a36b806d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
